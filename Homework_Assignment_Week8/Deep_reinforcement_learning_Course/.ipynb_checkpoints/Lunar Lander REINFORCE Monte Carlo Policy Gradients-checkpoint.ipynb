{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander: REINFORCE Monte Carlo Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll implement an agent <b>that plays Lunar Lander </b>\n",
    "\n",
    "<img src=\"http://gym.openai.com/v2018-02-21/videos/LunarLander-v2-b5632e53-9dbb-4135-bc4c-bee948450d63/poster.jpg\" alt=\"Lunar Lander\"/>\n",
    "\n",
    "## Lunar Lander\n",
    "* [https://gym.openai.com/envs/LunarLander-v2/](https://gym.openai.com/envs/LunarLander-v2/)\n",
    "* [https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py)\n",
    "* [https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2](https://github.com/openai/gym/wiki/Leaderboard#lunarlander-v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This solution is based on:\n",
    "* [https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb)\n",
    "\n",
    "## How to run\n",
    "\n",
    "```\n",
    "cd Homework_Assignment_Week8\n",
    "jupyter notebook Lunar\\ Lander\\ REINFORCE\\ Monte\\ Carlo\\ Policy\\ Gradients.ipynb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "This time we use <a href=\"https://gym.openai.com/\">OpenAI Gym</a> which has a lot of great environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env = env.unwrapped\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up our hyperparameters ‚öóÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ENVIRONMENT Hyperparameters\n",
    "state_size = 8\n",
    "action_size = env.action_space.n\n",
    "\n",
    "## TRAINING Hyperparameters\n",
    "max_episodes = 300\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95 # Discount rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Define the preprocessing functions ‚öôÔ∏è\n",
    "This function takes <b>the rewards and perform discounting.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Policy Gradient Neural Network model üß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "    with tf.name_scope(\"fc1\"):\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs = input_,\n",
    "                                                num_outputs = 10,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"fc2\"):\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    with tf.name_scope(\"fc3\"):\n",
    "        fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                                num_outputs = action_size,\n",
    "                                                activation_fn= None,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might now consider using \n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits(logits = fc3, labels = actions)\n",
    "        loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "        \n",
    "    \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/pg/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train our Agent üèÉ‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create the NN\n",
    "maxReward = 0 # Keep track of maximum reward\n",
    "For episode in range(max_episodes):\n",
    "    episode + 1\n",
    "    reset environment\n",
    "    reset stores (states, actions, rewards)\n",
    "    \n",
    "    For each step:\n",
    "        Choose action a\n",
    "        Perform action a\n",
    "        Store s, a, r\n",
    "        If done:\n",
    "            Calculate sum reward\n",
    "            Calculate gamma Gt\n",
    "            Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  0\n",
      "Reward:  -302.762409694\n",
      "Mean Reward -302.762409694\n",
      "Max reward so far:  -302.762409694\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  1\n",
      "Reward:  -137.125651132\n",
      "Mean Reward -219.944030413\n",
      "Max reward so far:  -137.125651132\n",
      "==========================================\n",
      "Episode:  2\n",
      "Reward:  -255.416011056\n",
      "Mean Reward -231.768023961\n",
      "Max reward so far:  -137.125651132\n",
      "==========================================\n",
      "Episode:  3\n",
      "Reward:  -146.269564031\n",
      "Mean Reward -210.393408978\n",
      "Max reward so far:  -137.125651132\n",
      "==========================================\n",
      "Episode:  4\n",
      "Reward:  -511.183935194\n",
      "Mean Reward -270.551514221\n",
      "Max reward so far:  -137.125651132\n",
      "==========================================\n",
      "Episode:  5\n",
      "Reward:  -701.296220035\n",
      "Mean Reward -342.342298524\n",
      "Max reward so far:  -137.125651132\n",
      "==========================================\n",
      "Episode:  6\n",
      "Reward:  -209.002647548\n",
      "Mean Reward -323.293776956\n",
      "Max reward so far:  -137.125651132\n",
      "==========================================\n",
      "Episode:  7\n",
      "Reward:  -13.3078446804\n",
      "Mean Reward -284.545535421\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  8\n",
      "Reward:  -344.340835517\n",
      "Mean Reward -291.189457654\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  9\n",
      "Reward:  -496.109205775\n",
      "Mean Reward -311.681432466\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  10\n",
      "Reward:  -147.623429534\n",
      "Mean Reward -296.767068563\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  11\n",
      "Reward:  -311.000341382\n",
      "Mean Reward -297.953174632\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  12\n",
      "Reward:  -569.582179203\n",
      "Mean Reward -318.847713445\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  13\n",
      "Reward:  -379.379015189\n",
      "Mean Reward -323.171377855\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  14\n",
      "Reward:  -245.362616502\n",
      "Mean Reward -317.984127098\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  15\n",
      "Reward:  -153.009729312\n",
      "Mean Reward -307.673227237\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  16\n",
      "Reward:  -426.245349102\n",
      "Mean Reward -314.648057935\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  17\n",
      "Reward:  -231.37286076\n",
      "Mean Reward -310.021658092\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  18\n",
      "Reward:  -124.603936525\n",
      "Mean Reward -300.262830641\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  19\n",
      "Reward:  -175.190183547\n",
      "Mean Reward -294.009198286\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  20\n",
      "Reward:  -128.250977941\n",
      "Mean Reward -286.115949698\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  21\n",
      "Reward:  -151.044455077\n",
      "Mean Reward -279.976336306\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  22\n",
      "Reward:  -277.928405774\n",
      "Mean Reward -279.887295848\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  23\n",
      "Reward:  -419.838790766\n",
      "Mean Reward -285.718608137\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  24\n",
      "Reward:  -569.668030832\n",
      "Mean Reward -297.076585044\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  25\n",
      "Reward:  -302.027994629\n",
      "Mean Reward -297.267023875\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  26\n",
      "Reward:  -379.566482509\n",
      "Mean Reward -300.315151972\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  27\n",
      "Reward:  -130.465901037\n",
      "Mean Reward -294.249107296\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  28\n",
      "Reward:  -215.149136701\n",
      "Mean Reward -291.521522103\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  29\n",
      "Reward:  -155.205591814\n",
      "Mean Reward -286.97765776\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  30\n",
      "Reward:  -251.631286776\n",
      "Mean Reward -285.837452244\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  31\n",
      "Reward:  -206.384912177\n",
      "Mean Reward -283.354560367\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  32\n",
      "Reward:  -249.269061375\n",
      "Mean Reward -282.321666458\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  33\n",
      "Reward:  -152.401230688\n",
      "Mean Reward -278.500477171\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  34\n",
      "Reward:  -414.912585586\n",
      "Mean Reward -282.397965983\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  35\n",
      "Reward:  -543.656240144\n",
      "Mean Reward -289.655140265\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  36\n",
      "Reward:  -204.175487035\n",
      "Mean Reward -287.344879367\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  37\n",
      "Reward:  -154.553501678\n",
      "Mean Reward -283.850369428\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  38\n",
      "Reward:  -206.00835104\n",
      "Mean Reward -281.854420238\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  39\n",
      "Reward:  -345.07123491\n",
      "Mean Reward -283.434840605\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  40\n",
      "Reward:  -395.782193894\n",
      "Mean Reward -286.175019954\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  41\n",
      "Reward:  -187.281284882\n",
      "Mean Reward -283.820407214\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  42\n",
      "Reward:  -415.701573773\n",
      "Mean Reward -286.887411087\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  43\n",
      "Reward:  -561.998623943\n",
      "Mean Reward -293.139938652\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  44\n",
      "Reward:  -460.89937819\n",
      "Mean Reward -296.867926198\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  45\n",
      "Reward:  -163.708157522\n",
      "Mean Reward -293.973148618\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  46\n",
      "Reward:  -167.97872974\n",
      "Mean Reward -291.292416301\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  47\n",
      "Reward:  -297.242009105\n",
      "Mean Reward -291.416366151\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  48\n",
      "Reward:  -285.178888061\n",
      "Mean Reward -291.28907068\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  49\n",
      "Reward:  -159.074890335\n",
      "Mean Reward -288.644787073\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  50\n",
      "Reward:  -167.636264269\n",
      "Mean Reward -286.27207094\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  51\n",
      "Reward:  -353.172068311\n",
      "Mean Reward -287.558609351\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  52\n",
      "Reward:  -368.994881375\n",
      "Mean Reward -289.095142785\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  53\n",
      "Reward:  -217.595080547\n",
      "Mean Reward -287.771067558\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  54\n",
      "Reward:  -233.866465533\n",
      "Mean Reward -286.790983885\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  55\n",
      "Reward:  -216.222300744\n",
      "Mean Reward -285.530828829\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  56\n",
      "Reward:  -166.661423344\n",
      "Mean Reward -283.445400663\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  57\n",
      "Reward:  -192.276557267\n",
      "Mean Reward -281.873524052\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  58\n",
      "Reward:  -228.591401541\n",
      "Mean Reward -280.97043723\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  59\n",
      "Reward:  -487.179743336\n",
      "Mean Reward -284.407258999\n",
      "Max reward so far:  -13.3078446804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  60\n",
      "Reward:  -163.21356072\n",
      "Mean Reward -282.42047706\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  61\n",
      "Reward:  -76.8689003683\n",
      "Mean Reward -279.105129049\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  62\n",
      "Reward:  -402.6325683\n",
      "Mean Reward -281.065882053\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  63\n",
      "Reward:  -159.331692383\n",
      "Mean Reward -279.163785339\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  64\n",
      "Reward:  -47.3474093222\n",
      "Mean Reward -275.597379554\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  65\n",
      "Reward:  -155.960522521\n",
      "Mean Reward -273.784699902\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  66\n",
      "Reward:  -40.4284576522\n",
      "Mean Reward -270.301770913\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  67\n",
      "Reward:  -161.436996753\n",
      "Mean Reward -268.700818352\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  68\n",
      "Reward:  -258.843014666\n",
      "Mean Reward -268.557951632\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  69\n",
      "Reward:  -432.987749365\n",
      "Mean Reward -270.906948742\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  70\n",
      "Reward:  -163.664234926\n",
      "Mean Reward -269.396487984\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  71\n",
      "Reward:  -177.113846808\n",
      "Mean Reward -268.114784635\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  72\n",
      "Reward:  -235.901912252\n",
      "Mean Reward -267.67351241\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  73\n",
      "Reward:  -150.404322339\n",
      "Mean Reward -266.088793626\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  74\n",
      "Reward:  -148.317439972\n",
      "Mean Reward -264.51850891\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  75\n",
      "Reward:  -197.062455599\n",
      "Mean Reward -263.630929261\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  76\n",
      "Reward:  -210.629810115\n",
      "Mean Reward -262.942603039\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  77\n",
      "Reward:  -131.396853458\n",
      "Mean Reward -261.25611907\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  78\n",
      "Reward:  -206.52924436\n",
      "Mean Reward -260.56337382\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  79\n",
      "Reward:  -346.627960871\n",
      "Mean Reward -261.639181158\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  80\n",
      "Reward:  -127.744455236\n",
      "Mean Reward -259.986159851\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  81\n",
      "Reward:  -159.818776782\n",
      "Mean Reward -258.764606399\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  82\n",
      "Reward:  -525.810288292\n",
      "Mean Reward -261.982024253\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  83\n",
      "Reward:  -475.77938203\n",
      "Mean Reward -264.527230893\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  84\n",
      "Reward:  -175.997395688\n",
      "Mean Reward -263.48570342\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  85\n",
      "Reward:  -123.831290452\n",
      "Mean Reward -261.861814897\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  86\n",
      "Reward:  -133.161089876\n",
      "Mean Reward -260.382496219\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  87\n",
      "Reward:  -271.526434736\n",
      "Mean Reward -260.509131884\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  88\n",
      "Reward:  -285.69847678\n",
      "Mean Reward -260.792158231\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  89\n",
      "Reward:  -166.151254613\n",
      "Mean Reward -259.740592635\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  90\n",
      "Reward:  -194.658681367\n",
      "Mean Reward -259.025406797\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  91\n",
      "Reward:  -428.740802054\n",
      "Mean Reward -260.870139354\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  92\n",
      "Reward:  -164.583337954\n",
      "Mean Reward -259.834797404\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  93\n",
      "Reward:  -138.011046154\n",
      "Mean Reward -258.53880005\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  94\n",
      "Reward:  -136.007115052\n",
      "Mean Reward -257.248992839\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  95\n",
      "Reward:  -153.242084362\n",
      "Mean Reward -256.165587543\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  96\n",
      "Reward:  -85.6004404028\n",
      "Mean Reward -254.407183964\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  97\n",
      "Reward:  -261.641836631\n",
      "Mean Reward -254.48100695\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  98\n",
      "Reward:  -235.297263488\n",
      "Mean Reward -254.287231764\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  99\n",
      "Reward:  -189.151423028\n",
      "Mean Reward -253.635873677\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  100\n",
      "Reward:  -171.888547587\n",
      "Mean Reward -252.82649421\n",
      "Max reward so far:  -13.3078446804\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  101\n",
      "Reward:  -184.946151138\n",
      "Mean Reward -252.161000651\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  102\n",
      "Reward:  -88.34892593\n",
      "Mean Reward -250.570592158\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  103\n",
      "Reward:  -200.762891157\n",
      "Mean Reward -250.091671956\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  104\n",
      "Reward:  -166.129729007\n",
      "Mean Reward -249.292034404\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  105\n",
      "Reward:  -334.95062731\n",
      "Mean Reward -250.100134338\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  106\n",
      "Reward:  -171.067954419\n",
      "Mean Reward -249.361515834\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  107\n",
      "Reward:  -389.76857083\n",
      "Mean Reward -250.661581158\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  108\n",
      "Reward:  -123.824429234\n",
      "Mean Reward -249.497937562\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  109\n",
      "Reward:  -65.7084291711\n",
      "Mean Reward -247.827123849\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  110\n",
      "Reward:  -266.071165057\n",
      "Mean Reward -247.991484581\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  111\n",
      "Reward:  -95.0148183532\n",
      "Mean Reward -246.62562149\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  112\n",
      "Reward:  -395.062960088\n",
      "Mean Reward -247.939226256\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  113\n",
      "Reward:  -225.885847374\n",
      "Mean Reward -247.745775564\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  114\n",
      "Reward:  -181.145546906\n",
      "Mean Reward -247.166643141\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  115\n",
      "Reward:  -257.998680494\n",
      "Mean Reward -247.260022773\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  116\n",
      "Reward:  -123.572254062\n",
      "Mean Reward -246.202862357\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  117\n",
      "Reward:  -217.930764688\n",
      "Mean Reward -245.963268309\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  118\n",
      "Reward:  -164.196018249\n",
      "Mean Reward -245.276148561\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  119\n",
      "Reward:  -347.471567145\n",
      "Mean Reward -246.127777049\n",
      "Max reward so far:  -13.3078446804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  120\n",
      "Reward:  -96.8525955229\n",
      "Mean Reward -244.894097863\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  121\n",
      "Reward:  -205.67722561\n",
      "Mean Reward -244.57264809\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  122\n",
      "Reward:  -266.491399299\n",
      "Mean Reward -244.750849319\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  123\n",
      "Reward:  -434.761184679\n",
      "Mean Reward -246.283190734\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  124\n",
      "Reward:  -274.981977375\n",
      "Mean Reward -246.512781027\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  125\n",
      "Reward:  -238.469845331\n",
      "Mean Reward -246.448948204\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  126\n",
      "Reward:  -171.797113957\n",
      "Mean Reward -245.861138485\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  127\n",
      "Reward:  -268.789647029\n",
      "Mean Reward -246.040267458\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  128\n",
      "Reward:  -245.489364415\n",
      "Mean Reward -246.035996892\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  129\n",
      "Reward:  -173.953523438\n",
      "Mean Reward -245.481516327\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  130\n",
      "Reward:  -80.284741352\n",
      "Mean Reward -244.220472243\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  131\n",
      "Reward:  -155.956013156\n",
      "Mean Reward -243.551802099\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  132\n",
      "Reward:  -234.231938422\n",
      "Mean Reward -243.481727936\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  133\n",
      "Reward:  -145.163896396\n",
      "Mean Reward -242.748012775\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  134\n",
      "Reward:  -262.751803924\n",
      "Mean Reward -242.896189006\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  135\n",
      "Reward:  -256.328175345\n",
      "Mean Reward -242.994953611\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  136\n",
      "Reward:  -191.34845528\n",
      "Mean Reward -242.617971871\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  137\n",
      "Reward:  -312.059111266\n",
      "Mean Reward -243.121168534\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  138\n",
      "Reward:  -236.083869014\n",
      "Mean Reward -243.07054048\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  139\n",
      "Reward:  -143.363613097\n",
      "Mean Reward -242.358348141\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  140\n",
      "Reward:  -183.43311287\n",
      "Mean Reward -241.940438671\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  141\n",
      "Reward:  -259.663466895\n",
      "Mean Reward -242.065248729\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  142\n",
      "Reward:  -116.487285645\n",
      "Mean Reward -241.187081155\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  143\n",
      "Reward:  -81.0538778188\n",
      "Mean Reward -240.075045021\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  144\n",
      "Reward:  -166.346191845\n",
      "Mean Reward -239.566570171\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  145\n",
      "Reward:  -283.901203433\n",
      "Mean Reward -239.870232043\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  146\n",
      "Reward:  -168.446002298\n",
      "Mean Reward -239.384352929\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  147\n",
      "Reward:  -302.420702543\n",
      "Mean Reward -239.81027421\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  148\n",
      "Reward:  -365.745951826\n",
      "Mean Reward -240.6554801\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  149\n",
      "Reward:  -133.391913832\n",
      "Mean Reward -239.940389658\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  150\n",
      "Reward:  -292.05170397\n",
      "Mean Reward -240.2854977\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  151\n",
      "Reward:  -143.991040885\n",
      "Mean Reward -239.651981537\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  152\n",
      "Reward:  -183.626447563\n",
      "Mean Reward -239.285801576\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  153\n",
      "Reward:  -134.556479754\n",
      "Mean Reward -238.605741045\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  154\n",
      "Reward:  -312.268810987\n",
      "Mean Reward -239.080986658\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  155\n",
      "Reward:  -296.370632994\n",
      "Mean Reward -239.44822798\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  156\n",
      "Reward:  -109.767398341\n",
      "Mean Reward -238.622235435\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  157\n",
      "Reward:  -183.4999238\n",
      "Mean Reward -238.273360045\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  158\n",
      "Reward:  -288.603901123\n",
      "Mean Reward -238.589904328\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  159\n",
      "Reward:  -300.751917296\n",
      "Mean Reward -238.978416909\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  160\n",
      "Reward:  -93.7795287763\n",
      "Mean Reward -238.076560461\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  161\n",
      "Reward:  -150.633890429\n",
      "Mean Reward -237.536790893\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  162\n",
      "Reward:  -291.047086274\n",
      "Mean Reward -237.865074914\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  163\n",
      "Reward:  -384.703433016\n",
      "Mean Reward -238.760430756\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  164\n",
      "Reward:  -204.561688504\n",
      "Mean Reward -238.553165651\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  165\n",
      "Reward:  -364.089813087\n",
      "Mean Reward -239.309410515\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  166\n",
      "Reward:  -359.088296334\n",
      "Mean Reward -240.026649353\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  167\n",
      "Reward:  -267.315060287\n",
      "Mean Reward -240.18908037\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  168\n",
      "Reward:  -164.389103269\n",
      "Mean Reward -239.740559796\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  169\n",
      "Reward:  -224.261971605\n",
      "Mean Reward -239.649509277\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  170\n",
      "Reward:  -111.179309028\n",
      "Mean Reward -238.898221556\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  171\n",
      "Reward:  -670.770294225\n",
      "Mean Reward -241.409105699\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  172\n",
      "Reward:  -312.657123188\n",
      "Mean Reward -241.820943951\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  173\n",
      "Reward:  -182.434629606\n",
      "Mean Reward -241.479643294\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  174\n",
      "Reward:  -119.656959788\n",
      "Mean Reward -240.783513674\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  175\n",
      "Reward:  -251.744754241\n",
      "Mean Reward -240.84579345\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  176\n",
      "Reward:  -213.733566211\n",
      "Mean Reward -240.692617025\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  177\n",
      "Reward:  -321.930667653\n",
      "Mean Reward -241.149010567\n",
      "Max reward so far:  -13.3078446804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  178\n",
      "Reward:  -361.906045639\n",
      "Mean Reward -241.823630875\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  179\n",
      "Reward:  -448.661826026\n",
      "Mean Reward -242.972731959\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  180\n",
      "Reward:  -416.139334898\n",
      "Mean Reward -243.929453522\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  181\n",
      "Reward:  -167.594842579\n",
      "Mean Reward -243.510032583\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  182\n",
      "Reward:  -464.440461916\n",
      "Mean Reward -244.717302689\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  183\n",
      "Reward:  -171.097898927\n",
      "Mean Reward -244.317197234\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  184\n",
      "Reward:  -387.562055758\n",
      "Mean Reward -245.091493766\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  185\n",
      "Reward:  -151.775118186\n",
      "Mean Reward -244.589792822\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  186\n",
      "Reward:  -261.952733281\n",
      "Mean Reward -244.682642771\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  187\n",
      "Reward:  -137.289900011\n",
      "Mean Reward -244.111404778\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  188\n",
      "Reward:  -162.132737408\n",
      "Mean Reward -243.677655215\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  189\n",
      "Reward:  -146.342968122\n",
      "Mean Reward -243.165367388\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  190\n",
      "Reward:  -199.64363167\n",
      "Mean Reward -242.937504897\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  191\n",
      "Reward:  -335.577953838\n",
      "Mean Reward -243.420007236\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  192\n",
      "Reward:  -117.432894865\n",
      "Mean Reward -242.76722427\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  193\n",
      "Reward:  -373.452479463\n",
      "Mean Reward -243.440859606\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  194\n",
      "Reward:  -356.439644828\n",
      "Mean Reward -244.020340556\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  195\n",
      "Reward:  -206.085806736\n",
      "Mean Reward -243.826797016\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  196\n",
      "Reward:  -306.975823993\n",
      "Mean Reward -244.147350453\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  197\n",
      "Reward:  -326.756472653\n",
      "Mean Reward -244.564568241\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  198\n",
      "Reward:  -406.939620037\n",
      "Mean Reward -245.380523276\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  199\n",
      "Reward:  -223.287361216\n",
      "Mean Reward -245.270057465\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  200\n",
      "Reward:  -207.375424927\n",
      "Mean Reward -245.081526955\n",
      "Max reward so far:  -13.3078446804\n",
      "Model saved\n",
      "==========================================\n",
      "Episode:  201\n",
      "Reward:  -179.23500062\n",
      "Mean Reward -244.755554052\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  202\n",
      "Reward:  -370.796728558\n",
      "Mean Reward -245.376446538\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  203\n",
      "Reward:  -423.922812717\n",
      "Mean Reward -246.251673823\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  204\n",
      "Reward:  -386.592162683\n",
      "Mean Reward -246.936261573\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  205\n",
      "Reward:  -146.474556431\n",
      "Mean Reward -246.448583393\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  206\n",
      "Reward:  -139.662920336\n",
      "Mean Reward -245.932710625\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  207\n",
      "Reward:  -282.729668871\n",
      "Mean Reward -246.109619078\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  208\n",
      "Reward:  -488.931330432\n",
      "Mean Reward -247.271445448\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  209\n",
      "Reward:  -173.249290921\n",
      "Mean Reward -246.918958998\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  210\n",
      "Reward:  -296.053752341\n",
      "Mean Reward -247.151825317\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  211\n",
      "Reward:  -146.708791637\n",
      "Mean Reward -246.678037422\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  212\n",
      "Reward:  -277.034370357\n",
      "Mean Reward -246.820555417\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  213\n",
      "Reward:  -177.347796911\n",
      "Mean Reward -246.495916359\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  214\n",
      "Reward:  -326.775265225\n",
      "Mean Reward -246.869308679\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  215\n",
      "Reward:  -322.557859298\n",
      "Mean Reward -247.219718636\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  216\n",
      "Reward:  -288.798813709\n",
      "Mean Reward -247.411327369\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  217\n",
      "Reward:  -199.562531901\n",
      "Mean Reward -247.191837481\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  218\n",
      "Reward:  -93.069249375\n",
      "Mean Reward -246.488081371\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  219\n",
      "Reward:  -495.855884226\n",
      "Mean Reward -247.621571384\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  220\n",
      "Reward:  -189.373723353\n",
      "Mean Reward -247.358006461\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  221\n",
      "Reward:  -353.546157888\n",
      "Mean Reward -247.836331467\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  222\n",
      "Reward:  -204.545085085\n",
      "Mean Reward -247.642200318\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  223\n",
      "Reward:  -179.182429637\n",
      "Mean Reward -247.336576341\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  224\n",
      "Reward:  -202.142326103\n",
      "Mean Reward -247.135713007\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  225\n",
      "Reward:  -352.585005291\n",
      "Mean Reward -247.602302796\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  226\n",
      "Reward:  -284.293991293\n",
      "Mean Reward -247.76394019\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  227\n",
      "Reward:  -421.523798315\n",
      "Mean Reward -248.526044831\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  228\n",
      "Reward:  -477.818611731\n",
      "Mean Reward -249.527322416\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  229\n",
      "Reward:  -226.099807818\n",
      "Mean Reward -249.425463657\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  230\n",
      "Reward:  -251.154206437\n",
      "Mean Reward -249.432947392\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  231\n",
      "Reward:  -365.645313576\n",
      "Mean Reward -249.933862763\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  232\n",
      "Reward:  -136.914043343\n",
      "Mean Reward -249.448799161\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  233\n",
      "Reward:  -435.938143523\n",
      "Mean Reward -250.245762171\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  234\n",
      "Reward:  -223.973423183\n",
      "Mean Reward -250.133964983\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  235\n",
      "Reward:  -384.357858687\n",
      "Mean Reward -250.702710296\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  236\n",
      "Reward:  -308.89359879\n",
      "Mean Reward -250.948241471\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  237\n",
      "Reward:  -243.146275644\n",
      "Mean Reward -250.915460102\n",
      "Max reward so far:  -13.3078446804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  238\n",
      "Reward:  -147.446827311\n",
      "Mean Reward -250.482536952\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  239\n",
      "Reward:  -133.462934127\n",
      "Mean Reward -249.994955274\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  240\n",
      "Reward:  -316.142980271\n",
      "Mean Reward -250.269428406\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  241\n",
      "Reward:  -141.390197586\n",
      "Mean Reward -249.819514229\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  242\n",
      "Reward:  -204.836499167\n",
      "Mean Reward -249.634398941\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  243\n",
      "Reward:  -203.067056534\n",
      "Mean Reward -249.443549177\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  244\n",
      "Reward:  -373.713306705\n",
      "Mean Reward -249.950772677\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  245\n",
      "Reward:  -143.507167536\n",
      "Mean Reward -249.518075095\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  246\n",
      "Reward:  -116.803343538\n",
      "Mean Reward -248.98076849\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  247\n",
      "Reward:  -120.998496705\n",
      "Mean Reward -248.464710942\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  248\n",
      "Reward:  -121.909046813\n",
      "Mean Reward -247.956455263\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  249\n",
      "Reward:  -139.195139077\n",
      "Mean Reward -247.521409998\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  250\n",
      "Reward:  -145.384199734\n",
      "Mean Reward -247.114488842\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  251\n",
      "Reward:  -251.544081065\n",
      "Mean Reward -247.132066589\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  252\n",
      "Reward:  -188.980365165\n",
      "Mean Reward -246.902217967\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  253\n",
      "Reward:  -137.843088152\n",
      "Mean Reward -246.472851314\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  254\n",
      "Reward:  -131.603960257\n",
      "Mean Reward -246.022385074\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  255\n",
      "Reward:  -151.637095411\n",
      "Mean Reward -245.653692537\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  256\n",
      "Reward:  -414.354382697\n",
      "Mean Reward -246.310115456\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  257\n",
      "Reward:  -123.113203535\n",
      "Mean Reward -245.832608045\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  258\n",
      "Reward:  -129.291183543\n",
      "Mean Reward -245.382641155\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  259\n",
      "Reward:  -225.952522406\n",
      "Mean Reward -245.307909929\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  260\n",
      "Reward:  -103.014116226\n",
      "Mean Reward -244.76272298\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  261\n",
      "Reward:  -458.535931218\n",
      "Mean Reward -245.578651256\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  262\n",
      "Reward:  -183.582913638\n",
      "Mean Reward -245.342926018\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  263\n",
      "Reward:  -244.132446203\n",
      "Mean Reward -245.338340867\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  264\n",
      "Reward:  -160.342973051\n",
      "Mean Reward -245.01760363\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  265\n",
      "Reward:  -108.904237613\n",
      "Mean Reward -244.505899246\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  266\n",
      "Reward:  -158.345023586\n",
      "Mean Reward -244.183199337\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  267\n",
      "Reward:  -145.280705233\n",
      "Mean Reward -243.81416018\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  268\n",
      "Reward:  -137.551662066\n",
      "Mean Reward -243.419132306\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  269\n",
      "Reward:  -149.17422701\n",
      "Mean Reward -243.070077101\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  270\n",
      "Reward:  -184.889878886\n",
      "Mean Reward -242.855390023\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  271\n",
      "Reward:  -144.327549153\n",
      "Mean Reward -242.493155314\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  272\n",
      "Reward:  -305.507011164\n",
      "Mean Reward -242.723975299\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  273\n",
      "Reward:  -384.737173816\n",
      "Mean Reward -243.242271644\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  274\n",
      "Reward:  -150.272899901\n",
      "Mean Reward -242.904201201\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  275\n",
      "Reward:  -130.865492559\n",
      "Mean Reward -242.498263851\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  276\n",
      "Reward:  -149.944094105\n",
      "Mean Reward -242.164133274\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  277\n",
      "Reward:  -225.692234215\n",
      "Mean Reward -242.104881839\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  278\n",
      "Reward:  -141.897968534\n",
      "Mean Reward -241.745717275\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  279\n",
      "Reward:  -298.59149262\n",
      "Mean Reward -241.948737901\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  280\n",
      "Reward:  -266.685566234\n",
      "Mean Reward -242.036769319\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  281\n",
      "Reward:  -242.883658084\n",
      "Mean Reward -242.03977247\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  282\n",
      "Reward:  -316.576082952\n",
      "Mean Reward -242.303151659\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  283\n",
      "Reward:  -209.698692848\n",
      "Mean Reward -242.188347227\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  284\n",
      "Reward:  -353.519925075\n",
      "Mean Reward -242.578984342\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  285\n",
      "Reward:  -334.823704997\n",
      "Mean Reward -242.901518331\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  286\n",
      "Reward:  -219.183695531\n",
      "Mean Reward -242.818877833\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  287\n",
      "Reward:  -81.7719914547\n",
      "Mean Reward -242.259687255\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  288\n",
      "Reward:  -249.745520522\n",
      "Mean Reward -242.285589793\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  289\n",
      "Reward:  -307.42660839\n",
      "Mean Reward -242.510213995\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  290\n",
      "Reward:  -458.763039849\n",
      "Mean Reward -243.253350853\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  291\n",
      "Reward:  -386.541362143\n",
      "Mean Reward -243.744063221\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  292\n",
      "Reward:  -437.873583984\n",
      "Mean Reward -244.406621312\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  293\n",
      "Reward:  -252.949157205\n",
      "Mean Reward -244.435677557\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  294\n",
      "Reward:  -150.950362025\n",
      "Mean Reward -244.118778182\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  295\n",
      "Reward:  -321.871516636\n",
      "Mean Reward -244.381456352\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  296\n",
      "Reward:  -223.31277722\n",
      "Mean Reward -244.310518039\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  297\n",
      "Reward:  -340.628683384\n",
      "Mean Reward -244.633733359\n",
      "Max reward so far:  -13.3078446804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Episode:  298\n",
      "Reward:  -216.345179745\n",
      "Mean Reward -244.539122811\n",
      "Max reward so far:  -13.3078446804\n",
      "==========================================\n",
      "Episode:  299\n",
      "Reward:  -397.185405387\n",
      "Mean Reward -245.047943753\n",
      "Max reward so far:  -13.3078446804\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "        \n",
    "        env.render()\n",
    "           \n",
    "        while True:\n",
    "            \n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,8])})\n",
    "            \n",
    "            # select action w.r.t the actions prob\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())\n",
    "\n",
    "            # Perform a\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            if done:\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"==========================================\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                                \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                loss_, _ = sess.run([loss, train_opt], feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                 discounted_episode_rewards_: discounted_episode_rewards \n",
    "                                                                })\n",
    "                \n",
    " \n",
    "                                                                 \n",
    "                # Write TF Summaries\n",
    "                #summary = sess.run(write_op, feed_dict={input_: np.vstack(np.array(episode_states)),\n",
    "                #                                                 actions: np.vstack(np.array(episode_actions)),\n",
    "                #                                                 discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                #                                                    mean_reward_: mean_reward\n",
    "                #                                                })\n",
    "                \n",
    "               \n",
    "                #writer.add_summary(summary, episode)\n",
    "                #writer.flush()\n",
    "                \n",
    "            \n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "        \n",
    "        # Save Model\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "Score -172.734114093\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Score -365.023196326\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "Score -308.516804824\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "Score -536.953320571\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "Score -229.889098342\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "Score -173.094697606\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "Score -235.598483917\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "Score -155.456255468\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "Score -208.079510543\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "Score -111.280167542\n",
      "Score over time: -249.662564923\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        while True:\n",
    "            \n",
    "\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1,8])})\n",
    "            #print(action_probability_distribution)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_rewards)\n",
    "                print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "    env.close()\n",
    "    print (\"Score over time: \" +  str(sum(rewards)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
